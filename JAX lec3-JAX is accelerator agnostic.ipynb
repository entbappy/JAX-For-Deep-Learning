{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"JAX lec3-JAX is accelerator agnostic.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNt+2HK9MuWTfgW7u5a59FD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":6,"metadata":{"id":"PlAGvOaq7qNd","executionInfo":{"status":"ok","timestamp":1641818804805,"user_tz":-360,"elapsed":925,"user":{"displayName":"Monika Yesmin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01725538247809880401"}}},"outputs":[],"source":["import jax.numpy as jnp\n","import numpy as np\n","\n","# Special transform functions (we'll understand what these are very soon!)\n","from jax import grad, jit, vmap, pmap\n","from jax import make_jaxpr\n","from jax import random\n","from jax import device_put\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["seed = 0\n","key = random.PRNGKey(seed)\n","\n","# Fact 4: JAX is AI accelerator agnostic. Same code runs everywhere!\n","\n","size = 3000\n","\n","# Data is automagically pushed to the AI accelerator! (DeviceArray structure)\n","# No more need for \".to(device)\" (PyTorch syntax)\n","x_jnp = random.normal(key, (size, size), dtype=jnp.float32)\n","x_np = np.random.normal(size=(size, size)).astype(np.float32)  # some diff in API exists!\n","\n","%timeit jnp.dot(x_jnp, x_jnp.T).block_until_ready()  # 1) on GPU - fast\n","%timeit np.dot(x_np, x_np.T)  # 2) on CPU - slow (NumPy only works with CPUs)\n","%timeit jnp.dot(x_np, x_np.T).block_until_ready()  # 3) on GPU with transfer overhead\n","\n","x_np_device = device_put(x_np)  # push NumPy explicitly to GPU\n","%timeit jnp.dot(x_np_device, x_np_device.T).block_until_ready()  # same as 1)\n","\n","# Note1: I'm using GPU as a synonym for AI accelerator. \n","# In reality, especially in Colab, this can also be a TPU, etc.\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XFEu2HNN78oG","executionInfo":{"status":"ok","timestamp":1641818998439,"user_tz":-360,"elapsed":12382,"user":{"displayName":"Monika Yesmin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01725538247809880401"}},"outputId":"11935e18-3064-45a3-c00a-cd5bb2d787d7"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["10 loops, best of 5: 25.9 ms per loop\n","1 loop, best of 5: 424 ms per loop\n","10 loops, best of 5: 92.9 ms per loop\n","10 loops, best of 5: 24.4 ms per loop\n"]}]},{"cell_type":"markdown","source":["- It is device-agnostic i.e. JAX doesn't need to track the device on which the array is present, and can avoid data transfers\n","\n","- Because it is device agnostic, this makes it easy to run the same JAX code on CPU, GPU, or TPU with no code changes"],"metadata":{"id":"ckBmCiY7O_At"}},{"cell_type":"code","source":[""],"metadata":{"id":"u5W3sY8D8WI3"},"execution_count":null,"outputs":[]}]}