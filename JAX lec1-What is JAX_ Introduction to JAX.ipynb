{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Introduction to JAX.ipynb","provenance":[],"authorship_tag":"ABX9TyOvd7Ragp78lvubOjwOPNlA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#What is the JAX ?"],"metadata":{"id":"HdqAhRvi8JHr"}},{"cell_type":"markdown","source":["<img src=\"https://raw.githubusercontent.com/google/jax/main/images/jax_logo_250px.png\" width=\"640\">"],"metadata":{"id":"1YiayVDJ62Vc"}},{"cell_type":"markdown","source":["### JAX is a Deep Learning Framework written by google\n","\n","JAX is Autograd and XLA, brought together for high-performance numerical computing and machine learning research. It provides composable transformations of Python+NumPy programs: differentiate, vectorize, parallelize, Just-In-Time compile to GPU/TPU, and more.\n","\n","[GITHUB Of JAX: ](https://github.com/google/jax)\n","\n","[JAX reference documentation](https://jax.readthedocs.io/en/latest/)\n","\n","\n","###XLA (accelerated linear algebra) is a compiler-based linear algebra execution engine. It is the backend that powers machine learning frameworks such as TensorFlow and JAX , on a variety of devices including CPUs, GPUs, and TPUs."],"metadata":{"id":"j8xDlCDk8r10"}},{"cell_type":"markdown","source":["You've probably heard of TensorFlow and PyTorch, and maybe you've even heard of MXNet - but there is a new kid on the block of machine learning frameworks - Google's JAX.\n","\n","Over the last two years, JAX has been taking deep learning research by storm, facilitating the implementation of Google's Vision Transformer (ViT) and powering research at DeepMind.\n","\n","So what is so exciting about the new JAX framework?"],"metadata":{"id":"A3wvqpmZ9OjX"}},{"cell_type":"markdown","source":["#JAX at Large\n","Boiled down, JAX is python's numpy with automatic differentiation and optimized to run on GPU. The seamless translation between writing numpy and writing in JAX has made JAX popular with machine learning practitioners.\n","\n","JAX offers four main function transformations that make it efficient to use when executing deep learning workloads."],"metadata":{"id":"-neyKpFC9mF7"}},{"cell_type":"markdown","source":["##JAX Four Function Transformations\n","#1.grad \n","- automatically differentiates a function for backpropagation. You can take grad to any derivative order."],"metadata":{"id":"xE2V3eFy94RD"}},{"cell_type":"code","source":["from jax import grad\n","import jax.numpy as jnp\n","\n","def tanh(x):  # Define a function\n","  y = jnp.exp(-2.0 * x)\n","  return (1.0 - y) / (1.0 + y)\n","\n","grad_tanh = grad(tanh)  # Obtain its gradient function\n","print(grad_tanh(1.0))   # Evaluate it at x = 1.0\n","# prints 0.4199743"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bqsns4fI5bMK","executionInfo":{"status":"ok","timestamp":1641734101346,"user_tz":-360,"elapsed":3343,"user":{"displayName":"Monika Yesmin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01725538247809880401"}},"outputId":"7dcced87-3e43-461a-c5af-93d9e56b42bf"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["0.4199743\n"]}]},{"cell_type":"markdown","source":["#2. jit \n","- auto-optimizes your functions to run their operations efficiently. Can also be used as a function decorator."],"metadata":{"id":"abp_fiiH-xdM"}},{"cell_type":"code","source":["import jax.numpy as jnp\n","from jax import jit\n","\n","def slow_f(x):\n","  # Element-wise ops see a large benefit from fusion\n","  return x * x + x * 2.0\n","\n","x = jnp.ones((5000, 5000))\n","fast_f = jit(slow_f)\n","%timeit -n10 -r3 fast_f(x)  \n","%timeit -n10 -r3 slow_f(x)  "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aTqQMUED-dR9","executionInfo":{"status":"ok","timestamp":1641734221468,"user_tz":-360,"elapsed":1317,"user":{"displayName":"Monika Yesmin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01725538247809880401"}},"outputId":"f8dc8fa2-1b7d-42ae-dc6c-3f5d0127d37e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["The slowest run took 239.21 times longer than the fastest. This could mean that an intermediate result is being cached.\n","10 loops, best of 3: 32.9 Âµs per loop\n","The slowest run took 4.95 times longer than the fastest. This could mean that an intermediate result is being cached.\n","10 loops, best of 3: 4.71 ms per loop\n"]}]},{"cell_type":"markdown","source":["#3. vmap \n","- maps a function across dimensions. Means that you don't have to keep track of dimensions as carefully when passing a batch through, for example."],"metadata":{"id":"mc0xxxi7_pIs"}},{"cell_type":"code","source":["predictions = vmap(predict, in_axes=(None, 0))(params, input_batch)"],"metadata":{"id":"2mxkuMU6_CcB","executionInfo":{"status":"ok","timestamp":1641734424655,"user_tz":-360,"elapsed":410,"user":{"displayName":"Monika Yesmin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01725538247809880401"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["#4. pmap \n","- maps processes across multiple processors, like multi-GPU"],"metadata":{"id":"ohDKVv75_5AT"}},{"cell_type":"code","source":["from jax import random, pmap\n","import jax.numpy as jnp\n","\n","# Create 8 random 5000 x 6000 matrices, one per GPU\n","keys = random.split(random.PRNGKey(0), 8)\n","mats = pmap(lambda key: random.normal(key, (5000, 6000)))(keys)\n","\n","# Run a local matmul on each device in parallel (no data transfer)\n","result = pmap(lambda x: jnp.dot(x, x.T))(mats)  # result.shape is (8, 5000, 5000)\n","\n","# Compute the mean on each device in parallel and print the result\n","print(pmap(jnp.mean)(result))\n","# prints [1.1566595 1.1805978 ... 1.2321935 1.2015157]"],"metadata":{"id":"Vut-9cNy_ynX","executionInfo":{"status":"ok","timestamp":1641734515377,"user_tz":-360,"elapsed":336,"user":{"displayName":"Monika Yesmin","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01725538247809880401"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["<img src=\"https://theaisummer.com/static/65961ba55109646b3aed515c7dba67cb/ee604/jax-tensorflow-pytorch.png\" width=\"740\">"],"metadata":{"id":"f2NIzJs5ARfN"}},{"cell_type":"markdown","source":["#JAX vs PyTorch\n","The nearest machine learning framework to JAX is PyTorch. That is because they share their roots in striving to be as \"numpy-esque\" as possible.\n","\n","JAX's functionality with lower level function definitions makes it preferrable for certain research tasks.\n","\n","That said, PyTorch offers a much further breadth of libraries and utilities, pre-trained and pre-written network definitions, a data loader, and portability to deployment destinations."],"metadata":{"id":"hI2f0Uj4A6BP"}},{"cell_type":"markdown","source":["#JAX vs TensorFlow\n","JAX and TensorFlow were both written by Google. From my initial experimentation, JAX seems much easier to develop in and is more intuitive.\n","\n","That said, JAX lacks the extensive infrastructure that TensorFlow has built over the years - be it open source projects, pre-trained models, tutorials, higher level abstractions (via Keras), and portability to deployment destinations."],"metadata":{"id":"F3ocisgYBbUD"}},{"cell_type":"markdown","source":["#What JAX lacks?\n","- A Data Loader - you'll need to implement your own or hop over to TensorFlow or PyTorch to borrow one,.\n","\n","- Higher level model abstractions\n","- Deployment portability"],"metadata":{"id":"57rRtVk6N9_g"}},{"cell_type":"markdown","source":["# When should we use JAX?\n","JAX is a new machine learning framework that has been gaining popularity in machine learning research.\n","\n","If you're operating in the research realm, JAX is a good option for your project.\n","\n","If you're actively developing an application, PyTorch and TensorFlow frameworks will move your initiative along with greater velocity. And of course, in computer vision there is always a tradeoff to weigh in building vs buying computer vision tooling.\n","\n","Thanks for reading on JAX! Happy Learning, and of course, happy inferencing!"],"metadata":{"id":"cyc9rgLXOPh8"}},{"cell_type":"code","source":[""],"metadata":{"id":"9FFQyNFiAF7Q"},"execution_count":null,"outputs":[]}]}